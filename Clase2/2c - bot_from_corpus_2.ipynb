{"cells":[{"cell_type":"markdown","metadata":{"id":"Ue5hxxkdAQJg"},"source":["<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n","\n","\n","# Procesamiento de lenguaje natural\n","## Bot con NLTK utilizando un corpus de wikipedia\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2873,"status":"ok","timestamp":1657553194174,"user":{"displayName":"Benjamin Tourn","userId":"03226511506067021855"},"user_tz":180},"id":"kCED1hh-Ioyf","outputId":"316f5481-ef7a-46ad-d624-7142cc057a33"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}],"source":["import json\n","import string\n","import random\n","import re\n","import urllib.request\n","\n","import numpy as np\n","\n","import nltk\n","# Descargar el diccionario\n","nltk.download(\"punkt\")\n","nltk.download(\"wordnet\")\n","nltk.download('omw-1.4')"]},{"cell_type":"markdown","metadata":{"id":"DMOa4JPSCJ29"},"source":["### Datos\n","Se consumira los datos del artículo de wikipedia sobre \"climate change\" en ingles."]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jF2p0dIX33OJ","executionInfo":{"status":"ok","timestamp":1657553278433,"user_tz":180,"elapsed":23015,"user":{"displayName":"Benjamin Tourn","userId":"03226511506067021855"}},"outputId":"3a6dd5fe-293f-477d-c845-68a19104f4fc"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/EIA-UBA/Bimestre3/NLP"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-5a6pl9D393w","executionInfo":{"status":"ok","timestamp":1657553306740,"user_tz":180,"elapsed":301,"user":{"displayName":"Benjamin Tourn","userId":"03226511506067021855"}},"outputId":"d3403168-9461-4ae5-c870-2f141b9078ac"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/EIA-UBA/Bimestre3/NLP\n"]}]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":279,"status":"ok","timestamp":1657553670089,"user":{"displayName":"Benjamin Tourn","userId":"03226511506067021855"},"user_tz":180},"id":"RIO7b8GjAC17"},"outputs":[],"source":["f = open(\"bible.txt\", \"r\")\n","article_text = f.read()"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":127,"output_embedded_package_id":"1MZlP4qu3W8mQww9E5Pv6Zo_KDe9RyZkh"},"executionInfo":{"elapsed":2109,"status":"ok","timestamp":1657553673790,"user":{"displayName":"Benjamin Tourn","userId":"03226511506067021855"},"user_tz":180},"id":"pUH30a1_rOkS","outputId":"b76f6d81-16f1-48e3-8ba4-bb21c327ebf7"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["# Demos un vistazo\n","article_text"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":272,"status":"ok","timestamp":1657553677446,"user":{"displayName":"Benjamin Tourn","userId":"03226511506067021855"},"user_tz":180},"id":"BtGLJjt6rQhK","outputId":"ceea96ad-ecef-4ad6-fb68-dcd7a8cc2498"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cantidad de caracteres en la nota: 4351186\n"]}],"source":["print(\"Cantidad de caracteres en la nota:\", len(article_text))"]},{"cell_type":"markdown","metadata":{"id":"FVHxBRNzCMOS"},"source":["### 2 - Preprocesamiento\n","- Remover caracteres especiales\n","- Quitar espacios o saltos"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":284,"status":"ok","timestamp":1657553681191,"user":{"displayName":"Benjamin Tourn","userId":"03226511506067021855"},"user_tz":180},"id":"HnEUTD1Erl1N"},"outputs":[],"source":["text = re.sub(r'\\[[0-9]*\\]', ' ', article_text)\n","text = re.sub(r'\\s+', ' ', text)"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":127,"output_embedded_package_id":"1MrpJMFOr0hyPOd6M-V2f06yaA8bawirV"},"executionInfo":{"elapsed":2274,"status":"ok","timestamp":1657553685361,"user":{"displayName":"Benjamin Tourn","userId":"03226511506067021855"},"user_tz":180},"id":"g7ycrAMYrn66","outputId":"745369a6-e7fe-49b4-927e-8995382d3871"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["# Demos un vistazo\n","text"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":293,"status":"ok","timestamp":1657553693002,"user":{"displayName":"Benjamin Tourn","userId":"03226511506067021855"},"user_tz":180},"id":"PA5F0s4UsMpf","outputId":"9513ce3b-c253-4ead-c036-afe704424da9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cantidad de caracteres en el texto: 4324027\n"]}],"source":["print(\"Cantidad de caracteres en el texto:\", len(text))"]},{"cell_type":"markdown","metadata":{"id":"DKNcDGcisajf"},"source":["### 3 - Dividir el texto en sentencias y en palabras"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":104655,"status":"ok","timestamp":1657553802459,"user":{"displayName":"Benjamin Tourn","userId":"03226511506067021855"},"user_tz":180},"id":"reXBOFQ7sdlB"},"outputs":[],"source":["corpus = nltk.sent_tokenize(text)\n","words = nltk.word_tokenize(text)"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":310,"status":"ok","timestamp":1657553804837,"user":{"displayName":"Benjamin Tourn","userId":"03226511506067021855"},"user_tz":180},"id":"J5GloV9fsi6o","outputId":"870c9f9a-4836-42b1-d8c5-3c86b18c9c49"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['1:1 In the beginning God created the heaven and the earth.',\n"," '1:2 And the earth was without form, and void; and darkness was upon the face of the deep.',\n"," 'And the Spirit of God moved upon the face of the waters.',\n"," '1:3 And God said, Let there be light: and there was light.',\n"," '1:4 And God saw the light, that it was good: and God divided the light from the darkness.',\n"," '1:5 And God called the light Day, and the darkness he called Night.',\n"," 'And the evening and the morning were the first day.',\n"," '1:6 And God said, Let there be a firmament in the midst of the waters, and let it divide the waters from the waters.',\n"," '1:7 And God made the firmament, and divided the waters which were under the firmament from the waters which were above the firmament: and it was so.',\n"," '1:8 And God called the firmament Heaven.']"]},"metadata":{},"execution_count":19}],"source":["# Demos un vistazo\n","corpus[:10]"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":313,"status":"ok","timestamp":1657553807635,"user":{"displayName":"Benjamin Tourn","userId":"03226511506067021855"},"user_tz":180},"id":"hmQ7nkvvsi0i","outputId":"1f0fd18d-0aad-44c7-f32b-f95280fcbc36"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['1:1',\n"," 'In',\n"," 'the',\n"," 'beginning',\n"," 'God',\n"," 'created',\n"," 'the',\n"," 'heaven',\n"," 'and',\n"," 'the',\n"," 'earth',\n"," '.',\n"," '1:2',\n"," 'And',\n"," 'the',\n"," 'earth',\n"," 'was',\n"," 'without',\n"," 'form',\n"," ',']"]},"metadata":{},"execution_count":20}],"source":["# Demos un vistazo\n","words[:20]"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":355,"status":"ok","timestamp":1657553813153,"user":{"displayName":"Benjamin Tourn","userId":"03226511506067021855"},"user_tz":180},"id":"YXPWNkKfEvDZ","outputId":"ff758010-cdb4-4ef2-8831-b12f312dce02"},"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulario: 950295\n"]}],"source":["print(\"Vocabulario:\", len(words))"]},{"cell_type":"markdown","metadata":{"id":"NlYKyb3OtDse"},"source":["### 4 - Funciones de ayuda para limpiar y procesar el input del usuario\n","- Lematizar los tokens de la oración\n","- Quitar símbolos de puntuación"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":275,"status":"ok","timestamp":1657553816946,"user":{"displayName":"Benjamin Tourn","userId":"03226511506067021855"},"user_tz":180},"id":"afPok8pstPOx"},"outputs":[],"source":["from nltk.stem import WordNetLemmatizer\n","lemmatizer = WordNetLemmatizer()\n","\n","def perform_lemmatization(tokens):\n","    return [lemmatizer.lemmatize(token) for token in tokens]\n","\n","punctuation_removal = dict((ord(punctuation), None) for punctuation in string.punctuation)\n","\n","def get_processed_text(document):\n","    # 1 - reduce el texto a mínuscula\n","    # 2 - quitar los simbolos de puntuacion\n","    # 3 - realiza la tokenización\n","    # 4 - realiza la lematización\n","    return perform_lemmatization(nltk.word_tokenize(document.lower().translate(punctuation_removal)))"]},{"cell_type":"markdown","metadata":{"id":"Jl8r6d9ZuyR9"},"source":["### 5 - Utilizar vectores TF-IDF y la similitud coseno construido con el corpus de wikipedia"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":285,"status":"ok","timestamp":1657553819716,"user":{"displayName":"Benjamin Tourn","userId":"03226511506067021855"},"user_tz":180},"id":"IRYFHcBfk2Gt"},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","def generate_response(user_input, corpus):\n","    response = ''\n","    # Sumar al corpus la pregunta del usuario para calcular\n","    # su cercania con otros documentos/sentencias\n","    corpus.append(user_input)\n","\n","    # Crear un vectorizar TFIDF que quite las \"stop words\" del ingles y utilice\n","    # nuestra funcion para obtener los tokens lematizados \"get_processed_text\"\n","    word_vectorizer = TfidfVectorizer(tokenizer=get_processed_text, stop_words='english')\n","\n","    # Crear los vectores a partir del corpus\n","    all_word_vectors = word_vectorizer.fit_transform(corpus)\n","\n","    # Calcular la similitud coseno entre todas los documentos excepto el agregado (el útlimo \"-1\")\n","    # NOTA: con los word embedings veremos más en detalle esta matriz de similitud\n","    similar_vector_values = cosine_similarity(all_word_vectors[-1], all_word_vectors)\n","\n","    # Obtener el índice del vector más cercano a nuestra oración\n","    # --> descartando la similitud contra nuestor vector propio\n","    similar_sentence_number = similar_vector_values.argsort()[0][-2]\n","    matched_vector = similar_vector_values.flatten()\n","    matched_vector.sort()\n","    vector_matched = matched_vector[-2]\n","\n","    if vector_matched == 0:\n","        response = \"I am sorry, I could not understand you\"\n","    else:\n","        response = corpus[similar_sentence_number]\n","    \n","    corpus.remove(user_input)\n","    return response"]},{"cell_type":"markdown","metadata":{"id":"OK-BuXPBybSp"},"source":["### 6 - Ensayar el sistema\n","El sistema intentará encontrar la parte del artículo que más se relaciona con nuestro texto de entrada. "]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25457,"status":"ok","timestamp":1657553848164,"user":{"displayName":"Benjamin Tourn","userId":"03226511506067021855"},"user_tz":180},"id":"Z2X4j8XyydSb","outputId":"c6e28779-90d2-43cb-b415-44e6f0acc46b"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 5.1 MB 28.7 MB/s \n","\u001b[K     |████████████████████████████████| 54 kB 2.8 MB/s \n","\u001b[K     |████████████████████████████████| 1.1 MB 35.1 MB/s \n","\u001b[K     |████████████████████████████████| 84 kB 2.9 MB/s \n","\u001b[K     |████████████████████████████████| 57 kB 4.6 MB/s \n","\u001b[K     |████████████████████████████████| 212 kB 34.5 MB/s \n","\u001b[K     |████████████████████████████████| 140 kB 54.9 MB/s \n","\u001b[K     |████████████████████████████████| 2.3 MB 47.9 MB/s \n","\u001b[K     |████████████████████████████████| 272 kB 50.4 MB/s \n","\u001b[K     |████████████████████████████████| 84 kB 3.7 MB/s \n","\u001b[K     |████████████████████████████████| 271 kB 60.3 MB/s \n","\u001b[K     |████████████████████████████████| 144 kB 47.6 MB/s \n","\u001b[K     |████████████████████████████████| 94 kB 2.8 MB/s \n","\u001b[K     |████████████████████████████████| 63 kB 1.7 MB/s \n","\u001b[K     |████████████████████████████████| 80 kB 8.1 MB/s \n","\u001b[K     |████████████████████████████████| 68 kB 6.2 MB/s \n","\u001b[K     |████████████████████████████████| 54 kB 3.2 MB/s \n","\u001b[K     |████████████████████████████████| 43 kB 1.9 MB/s \n","\u001b[K     |████████████████████████████████| 62 kB 737 kB/s \n","\u001b[K     |████████████████████████████████| 856 kB 54.9 MB/s \n","\u001b[K     |████████████████████████████████| 4.1 MB 36.7 MB/s \n","\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for python-multipart (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["# Se utilizará gradio para ensayar el bot\n","# Herramienta poderosa para crear interfaces rápidas para ensayar modelos\n","# https://gradio.app/\n","import sys\n","!{sys.executable} -m pip install gradio --quiet"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"SZv5MiVzynG1","executionInfo":{"status":"ok","timestamp":1657554219462,"user_tz":180,"elapsed":368363,"user":{"displayName":"Benjamin Tourn","userId":"03226511506067021855"}},"outputId":"ce9ecdd8-6660-4f1e-bca6-9282e9ce6d4d"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/gradio/deprecation.py:40: UserWarning: `layout` parameter is deprecated, and it has no effect\n","  warnings.warn(value)\n"]},{"output_type":"stream","name":"stdout","text":["Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n","Running on public URL: https://39082.gradio.app\n","\n","This share link expires in 72 hours. For free permanent hosting, check out Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://39082.gradio.app\" width=\"900\" height=\"500\" allow=\"autoplay; camera; microphone;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["god\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n","  % sorted(inconsistent)\n"]},{"output_type":"stream","name":"stdout","text":["jesus\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n","  % sorted(inconsistent)\n"]},{"output_type":"stream","name":"stdout","text":["apocalypsis\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n","  % sorted(inconsistent)\n"]},{"output_type":"stream","name":"stdout","text":["apocalypse\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n","  % sorted(inconsistent)\n"]},{"output_type":"stream","name":"stdout","text":["judea\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n","  % sorted(inconsistent)\n"]},{"output_type":"stream","name":"stdout","text":["king\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n","  % sorted(inconsistent)\n"]},{"output_type":"stream","name":"stdout","text":["son\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n","  % sorted(inconsistent)\n"]},{"output_type":"stream","name":"stdout","text":["earth\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n","  % sorted(inconsistent)\n"]},{"output_type":"stream","name":"stdout","text":["lord\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n","  % sorted(inconsistent)\n"]},{"output_type":"stream","name":"stdout","text":["Keyboard interruption in main thread... closing server.\n"]},{"output_type":"execute_result","data":{"text/plain":["(<gradio.routes.App at 0x7ff49161eb90>,\n"," 'http://127.0.0.1:7860/',\n"," 'https://39082.gradio.app')"]},"metadata":{},"execution_count":25}],"source":["import gradio as gr\n","\n","def bot_response(human_text):\n","    print(human_text)\n","    return generate_response(human_text.lower(), corpus)\n","\n","iface = gr.Interface(\n","    fn=bot_response,\n","    inputs=[\"textbox\"],\n","    outputs=\"text\",\n","    layout=\"vertical\")\n","\n","iface.launch(debug=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZloCqftOpWrD"},"outputs":[],"source":[""]}],"metadata":{"colab":{"collapsed_sections":[],"name":"2c - bot_from_corpus_2.ipynb","provenance":[{"file_id":"https://github.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/blob/main/clase_2/jupyter_notebooks/tensorflow/2c%20-%20bot_tfidf_nltk.ipynb","timestamp":1656979193874}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}